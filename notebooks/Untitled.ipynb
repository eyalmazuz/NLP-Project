{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 97,
   "id": "e06793f1",
   "metadata": {},
   "outputs": [],
   "source": [
    "from datasets import Dataset\n",
    "import evaluate\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "from transformers import AutoTokenizer, AutoModelForSeq2SeqLM, AutoModelForSequenceClassification\n",
    "from transformers import TrainingArguments, Trainer, IntervalStrategy\n",
    "from transformers import Seq2SeqTrainingArguments, Seq2SeqTrainer, DataCollatorForSeq2Seq\n",
    "\n",
    "from tqdm.auto import tqdm\n",
    "from sklearn.model_selection import LeavePGroupsOut\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "94ad816e",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv('../data/annotated_trees_101.csv', index_col=0)\n",
    "df.columns = ['node_id', 'tree_id', 'timestamp', 'author', 'text', 'parent',\n",
    "       'Aggressive', 'Agree But', 'Agree To Disagree', 'Alternative', 'Answer',\n",
    "       'Attack Validity', 'BAD', 'Clarification', 'Complaint', 'Convergence',\n",
    "       'Counter Argument', 'Critical Question', 'Direct No', 'Double Voicing',\n",
    "       'Extension', 'Irrelevance', 'Moderation', 'Neg Transformation',\n",
    "       'Nitpicking', 'No Reason Disagreement', 'Personal', 'Positive',\n",
    "       'Repetition', 'Rephrase Attack', 'Request Clarification', 'Ridicule',\n",
    "       'Sarcasm', 'Softening', 'Sources', 'Viable Transformation',\n",
    "       'W Qualifiers']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "id": "be62f83d",
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_post_comment_pairs(df: pd.DataFrame, task_type='t5') -> pd.DataFrame:\n",
    "    tuples = []\n",
    "    for row in tqdm(df.itertuples(), total=len(df)):\n",
    "        if row.parent == -1:\n",
    "            continue\n",
    "      \n",
    "        tree_id = row.tree_id\n",
    "        comment = row.text\n",
    "        root = df[(df['tree_id'] == tree_id) & (df['parent'] == -1)]['text'].values[0]\n",
    "          \n",
    "        if task_type == 't5':\n",
    "            tuples.append((root, comment, tree_id, row.timestamp, row.labels))\n",
    "\n",
    "        else:\n",
    "            tuples.append((root, comment, tree_id, row.timestamp, *row[7:]))\n",
    "    if task_type == 't5':        \n",
    "        tuples_df = pd.DataFrame(tuples, columns=['post', 'comment', 'tree_id', 'time', 'labels'])\n",
    "    else:\n",
    "        tuples_df = pd.DataFrame(tuples, columns=['post', 'comment', 'tree_id', 'time'] + df.columns[6:].tolist())\n",
    "    tuples_df['inputs'] = 'comment: ' + tuples_df.comment.str.cat(' post: ' + tuples_df.post)\n",
    "    \n",
    "    new_columns_order = tuples_df.columns[:4].tolist() + [tuples_df.columns[-1]] + tuples_df.columns[4:-1].tolist()\n",
    "    tuples_df = tuples_df[new_columns_order]\n",
    "\n",
    "    return tuples_df\n",
    "\n",
    "def remove_bad_comments(df: pd.DataFrame) -> pd.DataFrame:\n",
    "    removed_tokens = ['[removed]', '[deleted]']\n",
    "\n",
    "    df = df[~(df.post.isin(removed_tokens)) & ~(df.comment.isin(removed_tokens))]\n",
    "\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "id": "04468182",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "174f00c73f7e4262b2f3541a300f3bfd",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/10559 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "pairs_df = create_post_comment_pairs(df, 'bert')\n",
    "pairs_df = remove_bad_comments(pairs_df) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "id": "75d6e1d5",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>post</th>\n",
       "      <th>comment</th>\n",
       "      <th>tree_id</th>\n",
       "      <th>time</th>\n",
       "      <th>inputs</th>\n",
       "      <th>Aggressive</th>\n",
       "      <th>Agree But</th>\n",
       "      <th>Agree To Disagree</th>\n",
       "      <th>Alternative</th>\n",
       "      <th>Answer</th>\n",
       "      <th>...</th>\n",
       "      <th>Positive</th>\n",
       "      <th>Repetition</th>\n",
       "      <th>Rephrase Attack</th>\n",
       "      <th>Request Clarification</th>\n",
       "      <th>Ridicule</th>\n",
       "      <th>Sarcasm</th>\n",
       "      <th>Softening</th>\n",
       "      <th>Sources</th>\n",
       "      <th>Viable Transformation</th>\n",
       "      <th>W Qualifiers</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>I posted this on other subreddit but I figure ...</td>\n",
       "      <td>Are you talking about relationships starting a...</td>\n",
       "      <td>4r2a4d</td>\n",
       "      <td>1467557821</td>\n",
       "      <td>comment: Are you talking about relationships s...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>I posted this on other subreddit but I figure ...</td>\n",
       "      <td>I was focusing more on the first (relationship...</td>\n",
       "      <td>4r2a4d</td>\n",
       "      <td>1467558355</td>\n",
       "      <td>comment: I was focusing more on the first (rel...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>I posted this on other subreddit but I figure ...</td>\n",
       "      <td>I've been in a LDR for the past 2 1/2 years. W...</td>\n",
       "      <td>4r2a4d</td>\n",
       "      <td>1467584235</td>\n",
       "      <td>comment: I've been in a LDR for the past 2 1/2...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>I posted this on other subreddit but I figure ...</td>\n",
       "      <td>It depends on what people want. If you persona...</td>\n",
       "      <td>4r2a4d</td>\n",
       "      <td>1467559384</td>\n",
       "      <td>comment: It depends on what people want. If yo...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>I posted this on other subreddit but I figure ...</td>\n",
       "      <td>Agreed. But isn't companionship different in r...</td>\n",
       "      <td>4r2a4d</td>\n",
       "      <td>1467561555</td>\n",
       "      <td>comment: Agreed. But isn't companionship diffe...</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10453</th>\n",
       "      <td>Poverty Sensible Gun control Bigotry Racism Ob...</td>\n",
       "      <td>&lt;quote&gt;Also, there is no secret sauce teaching...</td>\n",
       "      <td>7yf2le</td>\n",
       "      <td>1519073471</td>\n",
       "      <td>comment: &lt;quote&gt;Also, there is no secret sauce...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10454</th>\n",
       "      <td>Poverty Sensible Gun control Bigotry Racism Ob...</td>\n",
       "      <td>you should teach!</td>\n",
       "      <td>7yf2le</td>\n",
       "      <td>1519073875</td>\n",
       "      <td>comment: you should teach! post: Poverty Sensi...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10455</th>\n",
       "      <td>Poverty Sensible Gun control Bigotry Racism Ob...</td>\n",
       "      <td>Nope there is even less money in that than the...</td>\n",
       "      <td>7yf2le</td>\n",
       "      <td>1519074041</td>\n",
       "      <td>comment: Nope there is even less money in that...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10456</th>\n",
       "      <td>Poverty Sensible Gun control Bigotry Racism Ob...</td>\n",
       "      <td>Ted Bundy had an IQ of 136. Would more educati...</td>\n",
       "      <td>7yf2le</td>\n",
       "      <td>1519067247</td>\n",
       "      <td>comment: Ted Bundy had an IQ of 136. Would mor...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10457</th>\n",
       "      <td>Poverty Sensible Gun control Bigotry Racism Ob...</td>\n",
       "      <td>There will always be outliers. I don’t think h...</td>\n",
       "      <td>7yf2le</td>\n",
       "      <td>1519067730</td>\n",
       "      <td>comment: There will always be outliers. I don’...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>9598 rows × 36 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                    post  \\\n",
       "0      I posted this on other subreddit but I figure ...   \n",
       "1      I posted this on other subreddit but I figure ...   \n",
       "2      I posted this on other subreddit but I figure ...   \n",
       "3      I posted this on other subreddit but I figure ...   \n",
       "4      I posted this on other subreddit but I figure ...   \n",
       "...                                                  ...   \n",
       "10453  Poverty Sensible Gun control Bigotry Racism Ob...   \n",
       "10454  Poverty Sensible Gun control Bigotry Racism Ob...   \n",
       "10455  Poverty Sensible Gun control Bigotry Racism Ob...   \n",
       "10456  Poverty Sensible Gun control Bigotry Racism Ob...   \n",
       "10457  Poverty Sensible Gun control Bigotry Racism Ob...   \n",
       "\n",
       "                                                 comment tree_id        time  \\\n",
       "0      Are you talking about relationships starting a...  4r2a4d  1467557821   \n",
       "1      I was focusing more on the first (relationship...  4r2a4d  1467558355   \n",
       "2      I've been in a LDR for the past 2 1/2 years. W...  4r2a4d  1467584235   \n",
       "3      It depends on what people want. If you persona...  4r2a4d  1467559384   \n",
       "4      Agreed. But isn't companionship different in r...  4r2a4d  1467561555   \n",
       "...                                                  ...     ...         ...   \n",
       "10453  <quote>Also, there is no secret sauce teaching...  7yf2le  1519073471   \n",
       "10454                                  you should teach!  7yf2le  1519073875   \n",
       "10455  Nope there is even less money in that than the...  7yf2le  1519074041   \n",
       "10456  Ted Bundy had an IQ of 136. Would more educati...  7yf2le  1519067247   \n",
       "10457  There will always be outliers. I don’t think h...  7yf2le  1519067730   \n",
       "\n",
       "                                                  inputs  Aggressive  \\\n",
       "0      comment: Are you talking about relationships s...           0   \n",
       "1      comment: I was focusing more on the first (rel...           0   \n",
       "2      comment: I've been in a LDR for the past 2 1/2...           0   \n",
       "3      comment: It depends on what people want. If yo...           0   \n",
       "4      comment: Agreed. But isn't companionship diffe...           0   \n",
       "...                                                  ...         ...   \n",
       "10453  comment: <quote>Also, there is no secret sauce...           0   \n",
       "10454  comment: you should teach! post: Poverty Sensi...           0   \n",
       "10455  comment: Nope there is even less money in that...           0   \n",
       "10456  comment: Ted Bundy had an IQ of 136. Would mor...           0   \n",
       "10457  comment: There will always be outliers. I don’...           0   \n",
       "\n",
       "       Agree But  Agree To Disagree  Alternative  Answer  ...  Positive  \\\n",
       "0              0                  0            0       0  ...         0   \n",
       "1              0                  0            0       0  ...         0   \n",
       "2              0                  0            0       0  ...         0   \n",
       "3              0                  0            0       0  ...         0   \n",
       "4              1                  0            0       0  ...         0   \n",
       "...          ...                ...          ...     ...  ...       ...   \n",
       "10453          0                  0            0       0  ...         0   \n",
       "10454          0                  0            0       0  ...         0   \n",
       "10455          0                  0            0       0  ...         1   \n",
       "10456          0                  0            0       0  ...         0   \n",
       "10457          0                  0            0       0  ...         0   \n",
       "\n",
       "       Repetition  Rephrase Attack  Request Clarification  Ridicule  Sarcasm  \\\n",
       "0               0                0                      1         0        0   \n",
       "1               0                0                      0         0        0   \n",
       "2               0                0                      0         0        0   \n",
       "3               0                0                      0         0        0   \n",
       "4               0                0                      0         0        0   \n",
       "...           ...              ...                    ...       ...      ...   \n",
       "10453           0                0                      0         0        0   \n",
       "10454           0                0                      0         0        1   \n",
       "10455           0                0                      0         0        0   \n",
       "10456           0                0                      0         1        0   \n",
       "10457           0                0                      0         0        0   \n",
       "\n",
       "       Softening  Sources  Viable Transformation  W Qualifiers  \n",
       "0              0        0                      0             0  \n",
       "1              0        0                      0             0  \n",
       "2              0        1                      0             0  \n",
       "3              0        0                      0             0  \n",
       "4              0        0                      0             0  \n",
       "...          ...      ...                    ...           ...  \n",
       "10453          0        0                      0             0  \n",
       "10454          0        0                      0             0  \n",
       "10455          0        0                      0             0  \n",
       "10456          0        0                      0             0  \n",
       "10457          0        0                      0             0  \n",
       "\n",
       "[9598 rows x 36 columns]"
      ]
     },
     "execution_count": 88,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pairs_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "bee5717e",
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_clf_tokenize_fn(tokenizer, source_max_legnth: int=512):\n",
    "    def tokenize_input(examples):\n",
    "        model_inputs = tokenizer(examples[\"inputs\"], max_length=source_max_legnth, padding=True, truncation=True)\n",
    "        model_inputs[\"labels\"] = examples.iloc[:, 4:]\n",
    "\n",
    "        return model_inputs\n",
    "\n",
    "    return tokenize_input"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "id": "042c41fd",
   "metadata": {},
   "outputs": [],
   "source": [
    "num_labels = pairs_df.shape[1] - 4\n",
    "id2label = {i: c for i, c in enumerate(pairs_df.columns[4:])}\n",
    "label2id = {c: i for i, c in id2label.items()}\n",
    "problem_type = 'multi_label_classification'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "id": "48893b25",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at bert-base-uncased were not used when initializing BertForSequenceClassification: ['cls.predictions.transform.LayerNorm.weight', 'cls.predictions.transform.dense.weight', 'cls.predictions.decoder.weight', 'cls.seq_relationship.weight', 'cls.seq_relationship.bias', 'cls.predictions.transform.dense.bias', 'cls.predictions.bias', 'cls.predictions.transform.LayerNorm.bias']\n",
      "- This IS expected if you are initializing BertForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing BertForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at bert-base-uncased and are newly initialized: ['classifier.weight', 'classifier.bias']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    }
   ],
   "source": [
    "model = AutoModelForSequenceClassification.from_pretrained('bert-base-uncased',\n",
    "                                                           problem_type=problem_type,\n",
    "                                                           num_labels=num_labels,\n",
    "                                                           id2label=id2label,\n",
    "                                                           label2id=label2id)\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained('bert-base-uncased')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "id": "7205af9d",
   "metadata": {},
   "outputs": [],
   "source": [
    "lpgo = LeavePGroupsOut(5)\n",
    "for (train_index, test_index) in lpgo.split(pairs_df, groups=pairs_df['tree_id']):\n",
    "    break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 117,
   "id": "3ccf61ea",
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_clf_tokenize_fn(tokenizer, labels, source_max_legnth: int=512):\n",
    "    def tokenize_input(examples):\n",
    "        model_inputs = tokenizer(examples[\"inputs\"], max_length=source_max_legnth, padding=True, truncation=True)\n",
    "        \n",
    "        labels_batch = {k: examples[k] for k in examples.keys() if k in labels}\n",
    "        # create numpy array of shape (batch_size, num_labels)\n",
    "        labels_matrix = np.zeros((len(examples[\"inputs\"]), len(labels)))\n",
    "        # fill numpy array\n",
    "        for idx, label in enumerate(labels):\n",
    "            labels_matrix[:, idx] = labels_batch[label]\n",
    "            \n",
    "        model_inputs[\"labels\"] = labels_matrix.tolist()\n",
    "\n",
    "        return model_inputs\n",
    "\n",
    "    return tokenize_input"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 118,
   "id": "e526ea11",
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer_fn = create_clf_tokenize_fn(tokenizer, pairs_df.columns[5:].tolist(), 512)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 119,
   "id": "68a64da2",
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "83f5944b45c4431c8aed528051c92928",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/10 [00:00<?, ?ba/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "31ffafe1a9164372b3429b40dd37074a",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/1 [00:00<?, ?ba/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "train_df = pairs_df.iloc[train_index]\n",
    "train_dataset = Dataset.from_pandas(train_df)\n",
    "train_dataset = train_dataset.map(tokenizer_fn, batched=True)\n",
    "\n",
    "test_df = pairs_df.iloc[test_index]\n",
    "test_dataset = Dataset.from_pandas(test_df)\n",
    "test_dataset = test_dataset.map(tokenizer_fn, batched=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 124,
   "id": "87831c05",
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(2, 31)"
      ]
     },
     "execution_count": 124,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ea5f876e",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
